---
layout: post
status: publish
published: true
title: How The Kernel Manages Your Memory
author: Gustavo Duarte
author_login: gduarte
author_email: gustavo-web@duartes.org
author_url: http://duartes.org/gustavo/blog
wordpress_id: 340
wordpress_url: http://duartes.org/gustavo/blog/?p=340
date: 2009-02-03 23:35:49.000000000 -07:00
comments: false
categories:
- Linux
- Software Illustrated
- Internals
tags: []
---
 <p>After examining the <a href="http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory">virtual address layout</a> of a process, we turn to the kernel and its mechanisms for managing user memory. Here is gonzo again:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/mm_struct.png" alt="Linux kernel mm_struct"/></p> <p>Linux processes are implemented in the kernel as instances of <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/sched.h#L1075">task_struct</a>, the process descriptor. The <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/sched.h#L1129">mm</a> field in task_struct points to the <strong>memory descriptor</strong>, <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L173">mm_struct</a>, which is an executive summary of a program's memory. It stores the start and end of memory segments as shown above, the <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L197">number</a> of physical memory pages used by the process (<strong>rss</strong> stands for Resident Set Size), the <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L206">amount</a> of virtual address space used, and other tidbits. Within the memory descriptor we also find the two work horses for managing program memory: the set of <strong>virtual memory areas</strong> and the <strong>page tables</strong>. Gonzo's memory areas are shown below:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/memoryDescriptorAndMemoryAreas.png" alt="Kernel memory descriptor and memory areas"/></p> <p>Each virtual memory area (VMA) is a contiguous range of virtual addresses; these areas never overlap. An instance of <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L99">vm_area_struct</a> fully describes a memory area, including its start and end addresses, <a href="http://lxr.linux.no/linux+v2.6.28/include/linux/mm.h#L76">flags</a> to determine access rights and behaviors, and the <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L150">vm_file</a> field to specify which file is being mapped by the area, if any. A VMA that does not map a file is <strong>anonymous</strong>. Each memory segment above (<em>e.g.</em>, heap, stack) corresponds to a single VMA, with the exception of the memory mapping segment. This is not a requirement, though it is usual in x86 machines. VMAs do not care which segment they are in.</p> <p>A program's VMAs are stored in its memory descriptor both as a linked list in the <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L174">mmap</a> field, ordered by starting virtual address, and as a <a href="http://en.wikipedia.org/wiki/Red_black_tree">red-black tree</a> rooted at the <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L175">mm_rb</a> field. The red-black tree allows the kernel to search quickly for the memory area covering a given virtual address. When you read file <tt>/proc/pid_of_process/maps</tt>, the kernel is simply going through the linked list of VMAs for the process and <a href="http://lxr.linux.no/linux+v2.6.28.1/fs/proc/task_mmu.c#L201">printing each one</a>.</p> <p>In Windows, the <a href="http://www.nirsoft.net/kernel_struct/vista/EPROCESS.html">EPROCESS</a> block is roughly a mix of task_struct and mm_struct. The Windows analog to a VMA is the Virtual Address Descriptor, or <a href="http://www.nirsoft.net/kernel_struct/vista/MMVAD.html">VAD</a>; they are stored in an <a href="http://en.wikipedia.org/wiki/AVL_tree">AVL tree</a>. You know what the funniest thing about Windows and Linux is? It's the little differences.</p> <p>The 4GB virtual address space is divided into <strong>pages</strong>. x86 processors in 32-bit mode support page sizes of 4KB, 2MB, and 4MB. Both Linux and Windows map the user portion of the virtual address space using 4KB pages. Bytes 0-4095 fall in page 0, bytes 4096-8191 fall in page 1, and so on. The size of a VMA <em>must be a multiple of page size</em>. Here's 3GB of user space in 4KB pages:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/pagedVirtualSpace.png" alt="4KB Pages Virtual User Space"/></p> <p>The processor consults <strong>page tables</strong> to translate a virtual address into a physical memory address. Each process has its own set of page tables; whenever a process switch occurs, page tables for user space are switched as well. Linux stores a pointer to a process' page tables in the <a href="http://lxr.linux.no/linux+v2.6.28.1/include/linux/mm_types.h#L185">pgd</a> field of the memory descriptor. To each virtual page there corresponds one <strong>page table entry</strong> (PTE) in the page tables, which in regular x86 paging is a simple 4-byte record shown below:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/x86PageTableEntry4KB.png" alt="x86 Page Table Entry (PTE) for 4KB page"/></p> <p>Linux has functions to <a href="http://lxr.linux.no/linux+v2.6.28.1/arch/x86/include/asm/pgtable.h#L173">read</a> and <a href="http://lxr.linux.no/linux+v2.6.28.1/arch/x86/include/asm/pgtable.h#L230">set</a> each flag in a PTE. Bit P tells the processor whether the virtual page is <strong>present</strong> in physical memory. If clear (equal to 0), accessing the page triggers a page fault. Keep in mind that when this bit is zero, <strong>the kernel can do whatever it pleases</strong> with the remaining fields. The R/W flag stands for read/write; if clear, the page is read-only. Flag U/S stands for user/supervisor; if clear, then the page can only be accessed by the kernel. These flags are used to implement the read-only memory and protected kernel space we saw before.</p> <p>Bits D and A are for <strong>dirty</strong> and <strong>accessed</strong>. A dirty page has had a write, while an accessed page has had a write or read. Both flags are sticky: the processor only sets them, they must be cleared by the kernel. Finally, the PTE stores the starting physical address that corresponds to this page, aligned to 4KB. This naive-looking field is the source of some pain, for it limits addressable physical memory to <a href="http://www.google.com/search?hl=en&amp;q=2^20+*+2^12+bytes+in+GB">4 GB</a>. The other PTE fields are for another day, as is Physical Address Extension.</p> <p>A virtual page is the unit of memory protection because all of its bytes share the U/S and R/W flags. However, the same physical memory could be mapped by different pages, possibly with different protection flags. Notice that execute permissions are nowhere to be seen in the PTE. This is why classic x86 paging allows code on the stack to be executed, making it easier to exploit stack buffer overflows (it's still possible to exploit non-executable stacks using <a href="http://en.wikipedia.org/wiki/Return-to-libc_attack">return-to-libc</a> and other techniques). This lack of a PTE no-execute flag illustrates a broader fact: permission flags in a VMA may or may not translate cleanly into hardware protection. The kernel does what it can, but ultimately the architecture limits what is possible.</p> <p>Virtual memory doesn't store anything, it simply <em>maps</em> a program's address space onto the underlying physical memory, which is accessed by the processor as a large block called the <strong>physical address space</strong>. While memory operations on the bus are <a href="http://duartes.org/gustavo/blog/post/getting-physical-with-memory">somewhat involved</a>, we can ignore that here and assume that physical addresses range from zero to the top of available memory in one-byte increments. This physical address space is broken down by the kernel into <strong>page frames</strong>. The processor doesn't know or care about frames, yet they are crucial to the kernel because <strong>the page frame is the unit of physical memory management.</strong> Both Linux and Windows use 4KB page frames in 32-bit mode; here is an example of a machine with 2GB of RAM:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/physicalAddressSpace.png" alt="Physical Address Space"/></p> <p>In Linux each page frame is tracked by a <a href="http://lxr.linux.no/linux+v2.6.28/include/linux/mm_types.h#L32">descriptor</a> and <a href="http://lxr.linux.no/linux+v2.6.28/include/linux/page-flags.h#L14">several flags</a>. Together these descriptors track the entire physical memory in the computer; the precise state of each page frame is always known. Physical memory is managed with the <a href="http://en.wikipedia.org/wiki/Buddy_memory_allocation">buddy memory allocation</a> technique, hence a page frame is <strong>free</strong> if it's available for allocation via the buddy system. An allocated page frame might be <strong>anonymous</strong>, holding program data, or it might be in the <strong>page cache</strong>, holding data stored in a file or block device. There are other exotic page frame uses, but leave them alone for now. Windows has an analogous Page Frame Number (PFN) database to track physical memory.</p> <p>Let's put together virtual memory areas, page table entries and page frames to understand how this all works. Below is an example of a user heap:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/heapMapped.png" alt="Physical Address Space"/></p> <p>Blue rectangles represent pages in the VMA range, while arrows represent page table entries mapping pages onto page frames. Some virtual pages lack arrows; this means their corresponding PTEs have the <strong>Present</strong> flag clear. This could be because the pages have never been touched or because their contents have been swapped out. In either case access to these pages will lead to page faults, even though they are within the VMA. It may seem strange for the VMA and the page tables to disagree, yet this often happens.</p> <p>A VMA is like a contract between your program and the kernel. You ask for something to be done (memory allocated, a file mapped, etc.), the kernel says "sure", and it creates or updates the appropriate VMA. But <em>it does not</em> actually honor the request right away, it waits until a page fault happens to do real work. The kernel is a lazy, deceitful sack of scum; this is the fundamental principle of virtual memory. It applies in most situations, some familiar and some surprising, but the rule is that VMAs record what has been <em>agreed upon</em>, while PTEs reflect what has <em>actually been done</em> by the lazy kernel. These two data structures together manage a program's memory; both play a role in resolving page faults, freeing memory, swapping memory out, and so on. Let's take the simple case of memory allocation:</p> <p align="center"><img src="http://static.duartes.org/img/blogPosts/heapAllocation.png" alt="Example of demand paging and memory allocation"/></p> <p>When the program asks for more memory via the <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/brk.2.html">brk()</a> system call, the kernel simply <a href="http://lxr.linux.no/linux+v2.6.28.1/mm/mmap.c#L2050">updates</a> the heap VMA and calls it good. No page frames are actually allocated at this point and the new pages are not present in physical memory. Once the program tries to access the pages, the processor page faults and <a href="http://lxr.linux.no/linux+v2.6.28/arch/x86/mm/fault.c#L583">do_page_fault()</a> is called. It <a href="http://lxr.linux.no/linux+v2.6.28/arch/x86/mm/fault.c#L692">searches</a> for the VMA covering the faulted virtual address using <a href="http://lxr.linux.no/linux+v2.6.28/mm/mmap.c#L1466">find_vma()</a>. If found, the permissions on the VMA are also checked against the attempted access (read or write). If there's no suitable VMA, no contract covers the attempted memory access and the process is punished by Segmentation Fault.</p> <p>When a VMA is <a href="http://lxr.linux.no/linux+v2.6.28/arch/x86/mm/fault.c#L711">found</a> the kernel must <a href="http://lxr.linux.no/linux+v2.6.28/mm/memory.c#L2653">handle</a> the fault by looking at the PTE contents and the type of VMA. In our case, the PTE shows the page is <a href="http://lxr.linux.no/linux+v2.6.28/mm/memory.c#L2674">not present</a>. In fact, our PTE is completely blank (all zeros), which in Linux means the virtual page has never been mapped. Since this is an anonymous VMA, we have a purely RAM affair that must be handled by <a href="http://lxr.linux.no/linux+v2.6.28/mm/memory.c#L2681">do_anonymous_page()</a>, which allocates a page frame and makes a PTE to map the faulted virtual page onto the freshly allocated frame.</p> <p>Things could have been different. The PTE for a swapped out page, for example, has 0 in the Present flag but is not blank. Instead, it stores the swap location holding the page contents, which must be read from disk and loaded into a page frame by <a href="http://lxr.linux.no/linux+v2.6.28/mm/memory.c#L2280">do_swap_page()</a> in what is called a <a href="http://lxr.linux.no/linux+v2.6.28/mm/memory.c#L2316">major fault</a>.</p> <p>This concludes the first half of our tour through the kernel's user memory management. In the next post, we'll throw files into the mix to build a complete picture of memory fundamentals, including consequences for performance.</p>

[124 Comments](/comments/kernel-memory.html)
